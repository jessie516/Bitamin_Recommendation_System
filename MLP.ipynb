{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced9a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d4391aa5e327>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown backend: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0m_SESSION\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0m_LEARNING_PHASE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'keras_learning_phase'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 0 = test, 1 = train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializations\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.models import Sequential, Graph, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from evaluate import evaluate_model\n",
    "from Dataset import Dataset\n",
    "from time import time\n",
    "import sys\n",
    "import argparse\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7941472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run MLP.\")\n",
    "    parser.add_argument('--path', nargs='?', default='Data/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--dataset', nargs='?', default='ml-1m',\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--layers', nargs='?', default='[64,32,16,8]',\n",
    "                        help=\"Size of each layer. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\")\n",
    "    parser.add_argument('--reg_layers', nargs='?', default='[0,0,0,0]',\n",
    "                        help=\"Regularization for each layer\")\n",
    "    parser.add_argument('--num_neg', type=int, default=4,\n",
    "                        help='Number of negative instances to pair with a positive instance.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Show performance per X iterations')\n",
    "    parser.add_argument('--out', type=int, default=1,\n",
    "                        help='Whether to save the trained model.')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal(shape, name=None):\n",
    "    return initializations.normal(shape, scale=0.01, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc473ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_users, num_items, layers = [20,10], reg_layers=[0,0]):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = layers[0]/2, name = 'user_embedding',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = layers[0]/2, name = 'item_embedding',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    \n",
    "    # The 0-th layer is the concatenation of embedding layers\n",
    "    vector = merge([user_latent, item_latent], mode = 'concat')\n",
    "    \n",
    "    # MLP layers\n",
    "    for idx in xrange(1, num_layer):\n",
    "        layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name = 'layer%d' %idx)\n",
    "        vector = layer(vector)\n",
    "        \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = 'prediction')(vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], \n",
    "                  output=prediction)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in xrange(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while train.has_key((u, j)):\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    path = args.path\n",
    "    dataset = args.dataset\n",
    "    layers = eval(args.layers)\n",
    "    reg_layers = eval(args.reg_layers)\n",
    "    num_negatives = args.num_neg\n",
    "    learner = args.learner\n",
    "    learning_rate = args.lr\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    verbose = args.verbose\n",
    "    \n",
    "    topK = 10\n",
    "    evaluation_threads = 1 #mp.cpu_count()\n",
    "    print(\"MLP arguments: %s \" %(args))\n",
    "    model_out_file = 'Pretrain/%s_MLP_%s_%d.h5' %(args.dataset, args.layers, time())\n",
    "    \n",
    "    # Loading data\n",
    "    t1 = time()\n",
    "    dataset = Dataset(args.path + args.dataset)\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    num_users, num_items = train.shape\n",
    "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "          %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
    "    \n",
    "    # Build model\n",
    "    model = get_model(num_users, num_items, layers, reg_layers)\n",
    "    if learner.lower() == \"adagrad\": \n",
    "        model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')    \n",
    "    \n",
    "    # Check Init performance\n",
    "    t1 = time()\n",
    "    (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print('Init: HR = %.4f, NDCG = %.4f [%.1f]' %(hr, ndcg, time()-t1))\n",
    "    \n",
    "    # Train model\n",
    "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "    for epoch in xrange(epochs):\n",
    "        t1 = time()\n",
    "        # Generate training instances\n",
    "        user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "    \n",
    "        # Training        \n",
    "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                         np.array(labels), # labels \n",
    "                         batch_size=batch_size, nb_epoch=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "\n",
    "        # Evaluation\n",
    "        if epoch %verbose == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "                  % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "                if args.out > 0:\n",
    "                    model.save_weights(model_out_file, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "    if args.out > 0:\n",
    "        print(\"The best MLP model is saved to %s\" %(model_out_file))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
