{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializations\n",
    "from keras.regularizers import l1, l2, l1l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from evaluate import evaluate_model\n",
    "from Dataset import Dataset\n",
    "from time import time\n",
    "import sys\n",
    "import GMF, MLP\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run NeuMF.\")\n",
    "    parser.add_argument('--path', nargs='?', default='Data/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--dataset', nargs='?', default='ml-1m',\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--num_factors', type=int, default=8,\n",
    "                        help='Embedding size of MF model.')\n",
    "    parser.add_argument('--layers', nargs='?', default='[64,32,16,8]',\n",
    "                        help=\"MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\")\n",
    "    parser.add_argument('--reg_mf', type=float, default=0,\n",
    "                        help='Regularization for MF embeddings.')                    \n",
    "    parser.add_argument('--reg_layers', nargs='?', default='[0,0,0,0]',\n",
    "                        help=\"Regularization for each MLP layer. reg_layers[0] is the regularization for embeddings.\")\n",
    "    parser.add_argument('--num_neg', type=int, default=4,\n",
    "                        help='Number of negative instances to pair with a positive instance.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Show performance per X iterations')\n",
    "    parser.add_argument('--out', type=int, default=1,\n",
    "                        help='Whether to save the trained model.')\n",
    "    parser.add_argument('--mf_pretrain', nargs='?', default='',\n",
    "                        help='Specify the pretrain model file for MF part. If empty, no pretrain will be used')\n",
    "    parser.add_argument('--mlp_pretrain', nargs='?', default='',\n",
    "                        help='Specify the pretrain model file for MLP part. If empty, no pretrain will be used')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal(shape, name=None):\n",
    "    return initializations.normal(shape, scale=0.01, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)   \n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = layers[0]/2, name = \"mlp_embedding_user\",\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = layers[0]/2, name = 'mlp_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)   \n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "    for idx in xrange(1, num_layer):\n",
    "        layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    #mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\n",
    "    #mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)\n",
    "    predict_vector = merge([mf_vector, mlp_vector], mode = 'concat')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], \n",
    "                  output=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_model(model, gmf_model, mlp_model, num_layers):\n",
    "    # MF embeddings\n",
    "    gmf_user_embeddings = gmf_model.get_layer('user_embedding').get_weights()\n",
    "    gmf_item_embeddings = gmf_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mf_embedding_user').set_weights(gmf_user_embeddings)\n",
    "    model.get_layer('mf_embedding_item').set_weights(gmf_item_embeddings)\n",
    "    \n",
    "    # MLP embeddings\n",
    "    mlp_user_embeddings = mlp_model.get_layer('user_embedding').get_weights()\n",
    "    mlp_item_embeddings = mlp_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mlp_embedding_user').set_weights(mlp_user_embeddings)\n",
    "    model.get_layer('mlp_embedding_item').set_weights(mlp_item_embeddings)\n",
    "    \n",
    "    # MLP layers\n",
    "    for i in xrange(1, num_layers):\n",
    "        mlp_layer_weights = mlp_model.get_layer('layer%d' %i).get_weights()\n",
    "        model.get_layer('layer%d' %i).set_weights(mlp_layer_weights)\n",
    "        \n",
    "    # Prediction weights\n",
    "    gmf_prediction = gmf_model.get_layer('prediction').get_weights()\n",
    "    mlp_prediction = mlp_model.get_layer('prediction').get_weights()\n",
    "    new_weights = np.concatenate((gmf_prediction[0], mlp_prediction[0]), axis=0)\n",
    "    new_b = gmf_prediction[1] + mlp_prediction[1]\n",
    "    model.get_layer('prediction').set_weights([0.5*new_weights, 0.5*new_b])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in xrange(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while train.has_key((u, j)):\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faff4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    num_epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    mf_dim = args.num_factors\n",
    "    layers = eval(args.layers)\n",
    "    reg_mf = args.reg_mf\n",
    "    reg_layers = eval(args.reg_layers)\n",
    "    num_negatives = args.num_neg\n",
    "    learning_rate = args.lr\n",
    "    learner = args.learner\n",
    "    verbose = args.verbose\n",
    "    mf_pretrain = args.mf_pretrain\n",
    "    mlp_pretrain = args.mlp_pretrain\n",
    "            \n",
    "    topK = 10\n",
    "    evaluation_threads = 1#mp.cpu_count()\n",
    "    print(\"NeuMF arguments: %s \" %(args))\n",
    "    model_out_file = 'Pretrain/%s_NeuMF_%d_%s_%d.h5' %(args.dataset, mf_dim, args.layers, time())\n",
    "\n",
    "    # Loading data\n",
    "    t1 = time()\n",
    "    dataset = Dataset(args.path + args.dataset)\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    num_users, num_items = train.shape\n",
    "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "          %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
    "    \n",
    "    # Build model\n",
    "    model = get_model(num_users, num_items, mf_dim, layers, reg_layers, reg_mf)\n",
    "    if learner.lower() == \"adagrad\": \n",
    "        model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')\n",
    "    \n",
    "    # Load pretrain model\n",
    "    if mf_pretrain != '' and mlp_pretrain != '':\n",
    "        gmf_model = GMF.get_model(num_users,num_items,mf_dim)\n",
    "        gmf_model.load_weights(mf_pretrain)\n",
    "        mlp_model = MLP.get_model(num_users,num_items, layers, reg_layers)\n",
    "        mlp_model.load_weights(mlp_pretrain)\n",
    "        model = load_pretrain_model(model, gmf_model, mlp_model, len(layers))\n",
    "        print(\"Load pretrained GMF (%s) and MLP (%s) models done. \" %(mf_pretrain, mlp_pretrain))\n",
    "        \n",
    "    # Init performance\n",
    "    (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print('Init: HR = %.4f, NDCG = %.4f' % (hr, ndcg))\n",
    "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "    if args.out > 0:\n",
    "        model.save_weights(model_out_file, overwrite=True) \n",
    "        \n",
    "    # Training model\n",
    "    for epoch in xrange(num_epochs):\n",
    "        t1 = time()\n",
    "        # Generate training instances\n",
    "        user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "        \n",
    "        # Training\n",
    "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                         np.array(labels), # labels \n",
    "                         batch_size=batch_size, nb_epoch=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        \n",
    "        # Evaluation\n",
    "        if epoch %verbose == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "                  % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "                if args.out > 0:\n",
    "                    model.save_weights(model_out_file, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "    if args.out > 0:\n",
    "        print(\"The best NeuMF model is saved to %s\" %(model_out_file))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
